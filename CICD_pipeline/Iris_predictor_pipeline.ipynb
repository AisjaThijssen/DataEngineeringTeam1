{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abce2fc7-7a08-46c0-90b1-8a8637d4b541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 1.8.14 requires google-cloud-storage<2,>=1.20.0, but you have google-cloud-storage 2.5.0 which is incompatible.\n",
      "google-cloud-pipeline-components 1.0.25 requires google-cloud-storage<2,>=1.20.0, but you have google-cloud-storage 2.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform {USER_FLAG} -q\n",
    "! pip3 install -U google-cloud-storage {USER_FLAG} -q\n",
    "! pip3 install {USER_FLAG} kfp google-cloud-pipeline-components --upgrade -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eaaab6-98ce-4c38-bb19-d054eb4bbaf0",
   "metadata": {},
   "source": [
    "## Restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a895652-0ee3-48a9-abcf-8fb92550c5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f100449-80ae-436e-8c5d-9dac2f5f45e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.dsl import (\n",
    "    Input,\n",
    "    Output,\n",
    "    Artifact,\n",
    "    Dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56aef78-72fc-436f-9cf4-a39f85e861f8",
   "metadata": {},
   "source": [
    "### Pipeline cloud parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d0e6209-3cb7-444a-868a-1a691aaf2126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Google Cloud project that this pipeline runs in.\n",
    "project_id = \"deassignement1\"\n",
    "# The region that this pipeline runs in\n",
    "region = \"us-central1\"\n",
    "# Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root.\n",
    "pipeline_root_path = \"gs://temp-storage-group1/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ba65a1-8598-4ccf-87f4-b610a895c30e",
   "metadata": {},
   "source": [
    "# Create Pipeline Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020dbab8-c2ab-4c3b-9e12-e6aebfb1a71e",
   "metadata": {},
   "source": [
    "#### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed9fd1b8-7d71-4502-91c0-19653cab61cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def download_data(project_id: str, bucket: str, file_name: str) -> Dict:\n",
    "    '''download data'''\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(bucket)\n",
    "    blob = bucket.blob(file_name)\n",
    "    local_path = '/tmp/'+ file_name\n",
    "    blob.download_to_filename(local_path)\n",
    "    logging.info('Downloaded Data!')\n",
    "    \n",
    "    # Convert the data to a dictiory object    \n",
    "    dict_from_csv = pd.read_csv(local_path, index_col=\"Id\", squeeze=True).to_dict()\n",
    "    logging.info('Returning Data as Dictionary Object!')\n",
    "    return dict_from_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5a13344-4eae-4fd8-aa9d-4989ba023714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a KFP component for data ingestion\n",
    "data_ingestion_comp = kfp.components.create_component_from_func(\n",
    "    download_data, output_component_file='data_ingestion.yaml', packages_to_install=['google-cloud-storage', 'pandas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fa2c92-4440-4de8-bc8f-3cbe7d9939bd",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0bed7dc-6b9e-4b43-8852-73804f4514b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, NamedTuple\n",
    "\n",
    "def make_train_test_split(input_data: Dict) -> NamedTuple(\n",
    "  'Outputs',\n",
    "  [\n",
    "    ('train_data', Dict),\n",
    "    ('test_data', Dict),\n",
    "  ]):\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import logging\n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    data = pd.DataFrame.from_dict(input_data) \n",
    "    \n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    logging.info(\"make train test split\")\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    example_output = namedtuple(\n",
    "      'Outputs',\n",
    "      ['train_data', 'test_data'])\n",
    "    return example_output(train_data.to_dict(), test_data.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f2c40a1-b59c-475f-9d5e-ad1ef024a3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a KFP component for data ingestion\n",
    "train_test_split_comp = kfp.components.create_component_from_func(\n",
    "    make_train_test_split, output_component_file='train_test_split.yaml', packages_to_install=['scikit-learn', 'pandas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b00e46-dca6-470b-a3ea-dff875e26249",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45fadaba-01e2-4695-a9fa-77dc8627c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "def train_decision_tree (features: Dict, project_id: str, model_repo: str) -> Dict:\n",
    "    '''train a Decision Tree with default parameters'''\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    import json\n",
    "    import logging \n",
    "    import sys\n",
    "    import os\n",
    "    import joblib\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(features)  \n",
    "    \n",
    "    logging.info(df.columns)\n",
    "        \n",
    "    # split into input (X) and output (y) variables\n",
    "    X = df.loc[:, ['SepalLengthCm','SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']].values\n",
    "    y = df.loc[:, ['Species']].values\n",
    "    \n",
    "    # define model\n",
    "    decision_tree_classifier = DecisionTreeClassifier(criterion = 'gini')\n",
    "    decision_tree_classifier.fit(X, y)\n",
    "\n",
    "    # evaluate the model\n",
    "    score = decision_tree_classifier.score(X, y)\n",
    "    logging.info(\"accuracy: \" + str(score))\n",
    "    metrics = {\n",
    "        \"accuracy\": score,\n",
    "    }\n",
    "   \n",
    "    # Save the model localy\n",
    "    local_file = '/tmp/DecisionTree.pkl'\n",
    "    joblib.dump(decision_tree_classifier, local_file)\n",
    "  \n",
    "    # Save to GCS as DecisionTree.pkl\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(model_repo)\n",
    "    blob = bucket.blob('DecisionTree.pkl')\n",
    "    # Upload the locally saved model\n",
    "    blob.upload_from_filename(local_file)\n",
    "\n",
    "    print(\"Saved the model to GCP bucket : \" + model_repo)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f09db978-c3b1-41e6-8165-ba8a4639a184",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_decision_tree_com = kfp.components.create_component_from_func(\n",
    "    train_decision_tree, output_component_file='training_decision_tree.yaml', packages_to_install=['google-cloud-storage', 'pandas', 'joblib', 'scikit-learn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6ec9bb6-957a-4541-bbf5-da63bbd8b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "def train_random_forest (features: Dict, project_id: str, model_repo: str) -> Dict:\n",
    "    '''train a Random Forest with default parameters'''\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import json\n",
    "    import logging \n",
    "    import sys\n",
    "    import os\n",
    "    import joblib\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(features)  \n",
    "    \n",
    "    logging.info(df.columns)\n",
    "        \n",
    "    # split into input (X) and output (y) variables\n",
    "    X = df.loc[:, ['SepalLengthCm','SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']].values\n",
    "    y = df.loc[:, ['Species']].values\n",
    "    \n",
    "    # define model\n",
    "    random_forest_classifier = RandomForestClassifier()\n",
    "    random_forest_classifier.fit(X, y)\n",
    "\n",
    "    # evaluate the model\n",
    "    score = random_forest_classifier.score(X, y)\n",
    "    logging.info(\"accuracy: \" + str(score))\n",
    "    metrics = {\n",
    "        \"accuracy\": score,\n",
    "    }\n",
    "   \n",
    "    # Save the model localy\n",
    "    local_file = '/tmp/RandomForest.pkl'\n",
    "    joblib.dump(random_forest_classifier, local_file)\n",
    "  \n",
    "    # Save to GCS as RandomForest.pkl\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(model_repo)\n",
    "    blob = bucket.blob('RandomForest.pkl')\n",
    "    # Upload the locally saved model\n",
    "    blob.upload_from_filename(local_file)\n",
    "\n",
    "    print(\"Saved the model to GCP bucket : \" + model_repo)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18f1bee6-40a9-433b-a9a1-fbd171f30b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_random_forest_com = kfp.components.create_component_from_func(\n",
    "    train_random_forest, output_component_file='training_random_forest.yaml', packages_to_install=['google-cloud-storage', 'pandas', 'joblib', 'scikit-learn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02da5a99-b620-463f-ba6a-fd6131bc9b80",
   "metadata": {},
   "source": [
    "#### Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7503cd78-86d7-4f62-aa78-4f6d6d084094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Dict\n",
    "\n",
    "def compare_model(dt_metric: Dict, rf_metric: Dict) -> str:\n",
    "    import logging\n",
    "    import json\n",
    "    import sys\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    logging.info(dt_metric)\n",
    "    logging.info(rf_metric)\n",
    "    if dt_metric.get(\"accuracy\") > rf_metric.get(\"accuracy\"):\n",
    "        return \"DT\"\n",
    "    else :\n",
    "        return \"RF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1352973-4916-40e2-a007-78e4c7483174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a KFP component for comparing DT and RF\n",
    "compare_model_com = kfp.components.create_component_from_func(\n",
    "    compare_model, output_component_file='model_selecion_com.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70040d08-f261-459a-af4b-cee462193e90",
   "metadata": {},
   "source": [
    "#### Test the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bea6f63-4dce-4e2d-8a5c-ef97ec49da4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "\n",
    "def test_decision_tree(project_id: str, model_repo: str, features: Dict) -> Dict:\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    from google.cloud import storage\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import json\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(features)    \n",
    "    \n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(model_repo)\n",
    "    blob = bucket.blob('DecisionTree.pkl')\n",
    "    filename = '/tmp/DecisionTree.pkl'\n",
    "    blob.download_to_filename(filename)\n",
    "        \n",
    "    #Loading the saved model with joblib\n",
    "    model = joblib.load(filename)\n",
    "\n",
    "    xNew = df[['SepalLengthCm','SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]\n",
    "\n",
    "    dfcp = df.copy()   \n",
    "    y_classes = model.predict(xNew)\n",
    "    logging.info(y_classes)\n",
    "    \n",
    "    accuracy = accuracy_score(df['Species'], y_classes)\n",
    "    logging.info(\"accuracy: \" + str(accuracy))\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "736bf552-d2db-409b-9423-9bdfb27e95d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a KFP component for prediction DT\n",
    "test_decision_tree_com = kfp.components.create_component_from_func(\n",
    "    test_decision_tree, output_component_file='prediction_decision_tree.yaml', packages_to_install=['google-cloud-storage', 'pandas', 'joblib', 'scikit-learn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09defce6-8c88-4617-b7e2-06300b299437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "\n",
    "def test_random_forest(project_id: str, model_repo: str, features: Dict) -> Dict:\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import joblib\n",
    "    from google.cloud import storage\n",
    "    import json\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(features)    \n",
    "    \n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(model_repo)\n",
    "    blob = bucket.blob('RandomForest.pkl')\n",
    "    filename = '/tmp/RandomForest.pkl'\n",
    "    blob.download_to_filename(filename)\n",
    "        \n",
    "    #Loading the saved model with joblib\n",
    "    model = joblib.load(filename)\n",
    "\n",
    "    xNew = df[['SepalLengthCm','SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]\n",
    "\n",
    "    dfcp = df.copy()   \n",
    "    y_classes = model.predict(xNew)\n",
    "    logging.info(y_classes)\n",
    "    \n",
    "    accuracy = accuracy_score(df['Species'], y_classes)\n",
    "    \n",
    "    logging.info(\"accuracy: \" + str(accuracy))\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1b0288d-0060-4ef7-97aa-855a6b6388a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a KFP component for prediction RF\n",
    "test_random_forest_com = kfp.components.create_component_from_func(\n",
    "    test_random_forest, output_component_file='prediction_random_forest.yaml', packages_to_install=['google-cloud-storage', 'pandas', 'joblib', 'scikit-learn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05cd041-3602-47dc-b70a-04e31cfabdf4",
   "metadata": {},
   "source": [
    "#### Save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7d36f39-5363-49e1-803e-7b801179d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def save_best_model(project_id: str, model_repo: str, best_model: str) -> None:\n",
    "    from google.cloud import storage\n",
    "    import sys\n",
    "    import os\n",
    "    \n",
    "    if best_model == \"RF\":\n",
    "        \n",
    "        client = storage.Client(project=project_id)\n",
    "        bucket = client.get_bucket(model_repo)\n",
    "        blob = bucket.blob('RandomForest.pkl')\n",
    "        filename = '/tmp/RandomForest.pkl'\n",
    "        blob.download_to_filename(filename)\n",
    "        \n",
    "        # Save to GCS\n",
    "        client = storage.Client(project=project_id)\n",
    "        bucket = client.get_bucket(model_repo)\n",
    "        blob = bucket.blob('BestModel.pkl')\n",
    "        blob.upload_from_filename(filename)\n",
    "        \n",
    "    if best_model == \"DT\":\n",
    "        client = storage.Client(project=project_id)\n",
    "        bucket = client.get_bucket(model_repo)\n",
    "        blob = bucket.blob('DecisionTree.pkl')\n",
    "        filename = '/tmp/DecisionTree.pkl'\n",
    "        blob.download_to_filename(filename)\n",
    "        \n",
    "        # Save to GCS\n",
    "        client = storage.Client(project=project_id)\n",
    "        bucket = client.get_bucket(model_repo)\n",
    "        blob = bucket.blob('BestModel.pkl')\n",
    "        blob.upload_from_filename(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed96aacf-489c-46cf-9d22-cfa21a08e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_best_model_com = kfp.components.create_component_from_func(\n",
    "    save_best_model, output_component_file='save_best_model.yaml', packages_to_install=['google-cloud-storage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba189b8d-1993-45a2-8583-9e4b4f580a54",
   "metadata": {},
   "source": [
    "#### Define The Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70fc4f68-160a-4372-8f72-347f92068924",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name=\"iris-predictor-pipeline\",\n",
    "    pipeline_root=pipeline_root_path)\n",
    "def pipeline(project_id: str, data_bucket: str, trainset_filename: str, model_repo: str):\n",
    "    \n",
    "    \n",
    "    di_op = data_ingestion_comp(\n",
    "        project_id=project_id,\n",
    "        bucket=data_bucket,\n",
    "        file_name=trainset_filename\n",
    "    )\n",
    "\n",
    "    train_test_split_op = train_test_split_comp(\n",
    "        input_data = di_op.output\n",
    "    ).after(di_op)\n",
    "\n",
    "    training_dt_job_run_op = train_decision_tree_com(\n",
    "        project_id=project_id,\n",
    "        model_repo=model_repo,       \n",
    "        features=train_test_split_op.outputs['train_data']\n",
    "    )\n",
    "    \n",
    "    training_rf_job_run_op = train_random_forest_com(\n",
    "        project_id=project_id,\n",
    "        model_repo=model_repo,       \n",
    "        features=train_test_split_op.outputs['train_data']\n",
    "    )\n",
    "    \n",
    "    test_dt_op = test_decision_tree_com(\n",
    "        project_id=project_id,\n",
    "        model_repo=model_repo,       \n",
    "        features=train_test_split_op.outputs['test_data']  \n",
    "    ).after(training_dt_job_run_op)\n",
    "    \n",
    "    test_rf_op = test_random_forest_com(\n",
    "        project_id=project_id,\n",
    "        model_repo=model_repo,       \n",
    "        features=train_test_split_op.outputs['test_data']\n",
    "    ).after(training_rf_job_run_op)\n",
    "    \n",
    "    compare_model_op = compare_model_com(\n",
    "        dt_metric=test_dt_op.output,\n",
    "        rf_metric=test_rf_op.output\n",
    "    )\n",
    "    \n",
    "    save_best_model_op = save_best_model_com(\n",
    "        project_id=project_id,\n",
    "        model_repo=model_repo,\n",
    "        best_model=compare_model_op.output\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b528ab6-5dff-45ae-8f7f-66de965c9036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='iris_predictor_training_pipeline.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdad31-2b62-4e5a-a359-a1b1b2fa2975",
   "metadata": {},
   "source": [
    "##### Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3afaf7a0-b4c1-4bd0-a128-3520f940b272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/406096928318/locations/us-central1/pipelineJobs/iris-predictor-pipeline-20221025080913\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/406096928318/locations/us-central1/pipelineJobs/iris-predictor-pipeline-20221025080913')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/iris-predictor-pipeline-20221025080913?project=406096928318\n",
      "PipelineJob projects/406096928318/locations/us-central1/pipelineJobs/iris-predictor-pipeline-20221025080913 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/406096928318/locations/us-central1/pipelineJobs/iris-predictor-pipeline-20221025080913 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/406096928318/locations/us-central1/pipelineJobs/iris-predictor-pipeline-20221025080913 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/406096928318/locations/us-central1/pipelineJobs/iris-predictor-pipeline-20221025080913 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/406096928318/locations/us-central1/pipelineJobs/iris-predictor-pipeline-20221025080913 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/406096928318/locations/us-central1/pipelineJobs/iris-predictor-pipeline-20221025080913 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/406096928318/locations/us-central1/pipelineJobs/iris-predictor-pipeline-20221025080913\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"iris-predictor\",\n",
    "    enable_caching=False,\n",
    "    template_path=\"iris_predictor_training_pipeline.json\",\n",
    "    pipeline_root=pipeline_root_path,\n",
    "    parameter_values={\n",
    "        'project_id': project_id, # makesure to use your project id \n",
    "        'data_bucket': 'data-group1',  # makesure to use your data bucket name \n",
    "        'trainset_filename': 'Iris.csv', # makesure to upload these to your data bucket\n",
    "        'model_repo':'model-repository-group1' # makesure to use your model bucket name \n",
    "    }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0289586e-a3f5-47d5-bb52-ea4c87b99c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce754976-09f5-47ca-8d36-51fc96ee2774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
