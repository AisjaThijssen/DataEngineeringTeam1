{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abce2fc7-7a08-46c0-90b1-8a8637d4b541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 1.8.14 requires google-cloud-storage<2,>=1.20.0, but you have google-cloud-storage 2.5.0 which is incompatible.\n",
      "google-cloud-pipeline-components 1.0.25 requires google-cloud-storage<2,>=1.20.0, but you have google-cloud-storage 2.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform {USER_FLAG} -q\n",
    "! pip3 install -U google-cloud-storage {USER_FLAG} -q\n",
    "! pip3 install {USER_FLAG} kfp google-cloud-pipeline-components --upgrade -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eaaab6-98ce-4c38-bb19-d054eb4bbaf0",
   "metadata": {},
   "source": [
    "## Restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a895652-0ee3-48a9-abcf-8fb92550c5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f100449-80ae-436e-8c5d-9dac2f5f45e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.dsl import (\n",
    "    Input,\n",
    "    Output,\n",
    "    Artifact,\n",
    "    Dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56aef78-72fc-436f-9cf4-a39f85e861f8",
   "metadata": {},
   "source": [
    "### Pipeline cloud parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d0e6209-3cb7-444a-868a-1a691aaf2126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Google Cloud project that this pipeline runs in.\n",
    "project_id = \"deassignement1\"\n",
    "# The region that this pipeline runs in\n",
    "region = \"us-central1\"\n",
    "# Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root.\n",
    "pipeline_root_path = \"gs://temp-storage-group1/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ba65a1-8598-4ccf-87f4-b610a895c30e",
   "metadata": {},
   "source": [
    "# Create Pipeline Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020dbab8-c2ab-4c3b-9e12-e6aebfb1a71e",
   "metadata": {},
   "source": [
    "#### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed9fd1b8-7d71-4502-91c0-19653cab61cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def download_data(project_id: str, bucket: str, file_name: str) -> Dict:\n",
    "    '''download data'''\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(bucket)\n",
    "    blob = bucket.blob(file_name)\n",
    "    local_path = '/tmp/'+ file_name\n",
    "    blob.download_to_filename(local_path)\n",
    "    logging.info('Downloaded Data!')\n",
    "    \n",
    "    # Convert the data to a dictiory object    \n",
    "    dict_from_csv = pd.read_csv(local_path, index_col=\"Id\", squeeze=True).to_dict()\n",
    "    logging.info('Returning Data as Dictionary Object!')\n",
    "    return dict_from_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5a13344-4eae-4fd8-aa9d-4989ba023714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a KFP component for data ingestion\n",
    "data_ingestion_comp = kfp.components.create_component_from_func(\n",
    "    download_data, output_component_file='data_ingestion.yaml', packages_to_install=['google-cloud-storage', 'pandas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b00e46-dca6-470b-a3ea-dff875e26249",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45fadaba-01e2-4695-a9fa-77dc8627c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Dict\n",
    "def train_decision_tree (features: Dict, project_id: str, model_repo: str) -> Dict:\n",
    "    '''train a Decision Tree with default parameters'''\n",
    "    import pandas as pd\n",
    "    from google.cloud import storage\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    import json\n",
    "    import logging \n",
    "    import sys\n",
    "    import os\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(features)  \n",
    "    \n",
    "    logging.info(df.columns)\n",
    "        \n",
    "    # split into input (X) and output (Y) variables\n",
    "    X = df.loc[:, ['SepalLengthCm','SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']].values\n",
    "    y = df.loc[:, ['Species']].values\n",
    "    \n",
    "    # define model\n",
    "    decision_tree_classifier = DecisionTreeClassifier(criterion = 'gini')\n",
    "    decision_tree_classifier.fit(X, y)\n",
    "\n",
    "    # evaluate the model\n",
    "    score = decision_tree_classifier.score(X, y)\n",
    "    logging.info(\"accuracy\")\n",
    "    metrics = {\n",
    "        \"accuracy\": score,\n",
    "    }\n",
    "   \n",
    "    # Save the model localy\n",
    "    local_file = '/tmp/DecisionTree.pkl'\n",
    "    joblib.dump(decision_tree_classifier, local_file)\n",
    "     # write out output\n",
    "  \n",
    "    # Save to GCS as model.h5\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(model_repo)\n",
    "    blob = bucket.blob('DecisionTree.pkl')\n",
    "    # Upload the locally saved model\n",
    "    blob.upload_from_filename(local_file)\n",
    "\n",
    "    print(\"Saved the model to GCP bucket : \" + model_repo)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f09db978-c3b1-41e6-8165-ba8a4639a184",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_decision_tree_com = kfp.components.create_component_from_func(\n",
    "    train_decision_tree, output_component_file='training_decision_tree.yaml', packages_to_install=['google-cloud-storage', 'pandas', 'joblib', 'scikit-learn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70040d08-f261-459a-af4b-cee462193e90",
   "metadata": {},
   "source": [
    "#### Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bea6f63-4dce-4e2d-8a5c-ef97ec49da4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_decision_tree(project_id: str, model_repo: str, features: Dict) -> Dict:\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    from google.cloud import storage\n",
    "    import json\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(features)    \n",
    "    \n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(model_repo)\n",
    "    blob = bucket.blob('DecisionTree.pkl')\n",
    "    filename = '/tmp/DecisionTree.pkl'\n",
    "    blob.download_to_filename(filename)\n",
    "        \n",
    "    #Loading the saved model with joblib\n",
    "    model = joblib.load(filename)\n",
    "\n",
    "    xNew = df[['SepalLengthCm','SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]\n",
    "\n",
    "    dfcp = df.copy()   \n",
    "    y_classes = model.predict(xNew)\n",
    "    logging.info(y_classes)\n",
    "    dfcp['pclass'] = y_classes.tolist()\n",
    "    dic = dfcp.to_dict(orient='records') \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "736bf552-d2db-409b-9423-9bdfb27e95d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a KFP component for prediction LR \n",
    "prediction_decision_tree = kfp.components.create_component_from_func(\n",
    "    predict_decision_tree, output_component_file='prediction_decision_tree.yaml', packages_to_install=['google-cloud-storage', 'pandas', 'joblib', 'scikit-learn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba189b8d-1993-45a2-8583-9e4b4f580a54",
   "metadata": {},
   "source": [
    "#### Define The Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70fc4f68-160a-4372-8f72-347f92068924",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name=\"iris-predictor-pipeline\",\n",
    "    pipeline_root=pipeline_root_path)\n",
    "def pipeline(project_id: str, data_bucket: str, trainset_filename: str, model_repo: str,): #testset_filename: str, ):\n",
    "    \n",
    "    \n",
    "    di_op = data_ingestion_comp(\n",
    "        project_id=project_id,\n",
    "        bucket=data_bucket,\n",
    "        file_name=trainset_filename\n",
    "    )\n",
    "\n",
    " \n",
    "    training_dt_job_run_op = train_decision_tree_com(\n",
    "        project_id=project_id,\n",
    "        model_repo=model_repo,       \n",
    "        features=di_op.output\n",
    "    )\n",
    "    \n",
    "    \"\"\"\n",
    "    pre_di_op = data_ingestion_comp(\n",
    "        project_id=project_id,\n",
    "        bucket=data_bucket,\n",
    "        file_name=testset_filename\n",
    "    ).after(training_mlp_job_run_op, training_lr_job_run_op)  \n",
    "    \n",
    "    # defining the branching condition\n",
    "    with dsl.Condition(comp_model__op.output==\"MLP\"):\n",
    "        predict_mlp_job_run_op = prediction_mpl_com(\n",
    "        project_id=project_id,\n",
    "        model_repo=model_repo,       \n",
    "        features=pre_di_op.output\n",
    "        )\n",
    "    with dsl.Condition(comp_model__op.output==\"LR\"):\n",
    "        predict_lr_job_run_op = prediction_lr_com(\n",
    "        project_id=project_id,\n",
    "        model_repo=model_repo,       \n",
    "        features=pre_di_op.output\n",
    "       )\n",
    "       \n",
    "       \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b528ab6-5dff-45ae-8f7f-66de965c9036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='iris_predictor_training_pipeline.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdad31-2b62-4e5a-a359-a1b1b2fa2975",
   "metadata": {},
   "source": [
    "##### Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afaf7a0-b4c1-4bd0-a128-3520f940b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"iris-predictor\",\n",
    "    enable_caching=False,\n",
    "    template_path=\"iris_predictor_training_pipeline.json\",\n",
    "    pipeline_root=pipeline_root_path,\n",
    "    parameter_values={\n",
    "        'project_id': project_id, # makesure to use your project id \n",
    "        'data_bucket': 'data-group1',  # makesure to use your data bucket name \n",
    "        'trainset_filename': 'Iris.csv',     # makesure to upload these to your data bucket from DE2022/lab4/data\n",
    "     #   'testset_filename': 'prediction_set.csv',    # makesure to upload these to your data bucket from DE2022/lab4/data\n",
    "        'model_repo':'model-repository-group1' # makesure to use your model bucket name \n",
    "    }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
